{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with Pandas\n",
    "\n",
    "* Matt Burton \n",
    "* Data Dojo \n",
    "* November 13th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Data Science with Python\n",
    "\n",
    "* Python is a very popular programming language for *doing* data science\n",
    "* Is a powerful and expressive interpreted programming lanaguage\n",
    "* Is fast enough for many data processing tasks\n",
    "* Can hook into lower level lanaguages like C and FORTRAN when necessary\n",
    "* Has a HUGE user community and many powerful 3rd party libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential Python Libraries for Data Science\n",
    "\n",
    "* **NumPy** - A low-level numerical computing library with a fast multidimensional array object *ndarray*\n",
    "* **pandas** - A higher level library with several user-friendly data structures for numerical computing and data processing. \n",
    "* **matplotlib** - The most used (but not necessarily loved) Python library for data visualizations. \n",
    "* **Jupyter** - A platform for interactive computing and data analysis. Allows for the creation of *notebooks* (like this one here) for conducting and publishing data workflows. IT IS GREAT!!!\n",
    "* **scikit-learn** - The go-to library for machine learning in Python. Implements many popular ML algorithms, has a nice API, and has many useful helper fuctions.\n",
    "* **statsmodel** - A library for \"classical\" (frequentist) statistics (think ANOVA). Mirrors many of the models in R. \n",
    "\n",
    "\n",
    "* All of these libraries work well together making the Python data sceince ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dive into Pandas\n",
    "\n",
    "\n",
    "* Pandas is a third party library for doing data analysis\n",
    "* It is a foundational component of Python data science\n",
    "* Developed by someone in the finance industry, but is now used by everyone\n",
    "* Vanilla Python can do many of the same things, but Pandas is *faster*\n",
    "* The core of Pandas are the data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Data Structures\n",
    "\n",
    "* To understand Pandas, which is hard, you need to start with three data structures\n",
    "    * Series - For one dimensional data\n",
    "    * Dataframe - For two dimensional data\n",
    "    * Index - For naming, selecting, and transforming data within a Pandas Series or Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "* A one-dimensional array of indexed data\n",
    "* Kind of like a blend of a Python list and dictionary\n",
    "* You can create them from a Python list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [0.25, 0.5, 0.75, 1.0]\n",
    "data = pd.Series(my_list)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can index a Series just like a list\n",
    "* Use index notation to grab the 2nd element of `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember, index by zero so 1 is the second element\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can also slice Series as well\n",
    "* Use slices to grab the 2nd and 3rd elements of this series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the 2nd & 3rd elements \n",
    "data[1:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Series also act like Python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use indexing and slicing like above, but now with keys instead of numbers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population['California']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Like a Python dictionary, a Series is a list of key/value pairs\n",
    "* But these are *ordered*, which means you can do slicing\n",
    "* Try slicing this series, but with keys instead of numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the same : notation, but use the state names listed above\n",
    "# Your code here:\n",
    "population.loc['California':'Illinois']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a couple ways of creating `Series` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a list with an implicit index\n",
    "pd.Series([2, 4, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a list with an *explicit* index\n",
    "pd.Series([2, 4, 6], index=['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a dictionary so keys are the index and get sorded by keys\n",
    "pd.Series({2:'a', 1:'b', 3:'c'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "* `DataFrames` are the real workhorse of Pandas and Python Data Science\n",
    "* We will be spending a lot of time with data inside of Dataframes, so buckle up!\n",
    "* `DataFrames` contain two-dimensional data, just like an Excel spreadsheet\n",
    "* In practice, a `DataFrame` is a bunch of `Series` lined up next to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with our population Series define above\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create an area Series\n",
    "area_dict = {'Illinois': 149995, 'California': 423967, \n",
    "             'Texas': 695662, 'Florida': 170312, \n",
    "             'New York': 141297}\n",
    "area = pd.Series(area_dict)\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now mash them together into a DataFrame\n",
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area}   )\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas automatically lines everything up because they have shared index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(area.index)\n",
    "print(population.index)\n",
    "print(states.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A `DataFrame` actually has two indexes\n",
    "* One for the rows (as seen above)\n",
    "* An another for the columes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes\n",
    "\n",
    "* Pandas `Series` and `DataFrames` are containers for data\n",
    "* Index (and Indexing) are the mechanism to make that data retrievable\n",
    "* In a `Series` the index is the key to each value in the list\n",
    "* In a `DataFrame` the index is the column headers, but also row headers\n",
    "* Indexing allows you to merge or join disparate datasets together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world data processing \n",
    "\n",
    "Let's write script that parses information out of an mbox email archive, `mbox-short.txt`, and put it into a Pandas Dataframe.\n",
    "\n",
    "* Parse every piece of information into a dictionary\n",
    "* Aggregate all of those dictionaries into a list\n",
    "* Create a Pandas DataFrame from that list of dictionaries\n",
    "\n",
    "\n",
    "So we will transform this:\n",
    "```\n",
    "From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008\n",
    "```\n",
    "into this:\n",
    "```\n",
    "{'year': '2008', 'month': 'Jan', 'dayofweek': 'Sat', 'address': 'stephen.marquard@uct.ac.za', 'day': '5', 'time': '09:14:16'}\n",
    "```\n",
    "into this:\n",
    "```\n",
    "address      stephen.marquard@uct.ac.za\n",
    "day                                   5\n",
    "dayofweek                           Sat\n",
    "month                               Jan\n",
    "time                           09:14:16\n",
    "year                               2008\n",
    "Name: 0, dtype: object\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the data manually with [this link](http://www.py4e.com/code3/mbox.txt) or run the cell below if you are on a Unix based system\n",
    "* If you are running this with Binder the data should already be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download the data\n",
    "!wget -nv https://www.py4e.com/code3/mbox.txt -O mbox.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!head -n 10 mbox.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What we want to do is parse the text file above into the nicely structured data below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'year': '2008', 'month': 'Jan', 'dayofweek': 'Sat', 'address': 'stephen.marquard@uct.ac.za', 'day': '5', 'time': '09:14:16'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty code that parses the mbox file\n",
    "with open(\"mbox.txt\", encoding=\"utf-8\") as email_file:\n",
    "    # create a list to contain all the data\n",
    "    # list comprehensions foo\n",
    "    email_data = [line.split()[1:7] \n",
    "                  for line in email_file \n",
    "                  if \"From \" in line]\n",
    "     \n",
    "cols = [\"address\", \n",
    "         \"dayofweek\",\n",
    "         \"month\",\n",
    "         \"day\",\n",
    "         \"time\",\n",
    "         \"year\"]\n",
    "emails_dataframe = pd.DataFrame(email_data, columns=cols)\n",
    "emails_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Once your data is in a Pandas `DataFrame` you can easily use a ton of analytical tools\n",
    "* You just have to get your data to fit into a dataframe\n",
    "* Getting data to fit is a big part of the \"data janitor\" work...it is the craft of data carpentry\n",
    "* However, as we will see, there is still a lot of carpentry work to do once your data fits into a `DataFrame`\n",
    "* This dataframe allows us ask questions of the data, if you know how to ask.\n",
    "* `value_counts()` is a `Series` method that tabulates the number of values.\n",
    "* First we need to extract the column we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_dataframe['dayofweek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_dataframe['dayofweek'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized String Operations\n",
    "\n",
    "* There is a Pandas way of doing this that is much more terse and compact\n",
    "* Pandas has a set of String operations that do much painful work for you\n",
    "* Especially handling bad data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So now lets try tabulating the number of institutions the Pandas way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a vectorized string operation over the email addresses\n",
    "emails_dataframe['address'].str.split(\"@\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have a Series of list objects (you can tell from the square brackets)\n",
    "* Lets get just the 2nd element of those lists. We can do that with [vectorized item access](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb#Vectorized-item-access-and-slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "emails_dataframe['address'].str.split(\"@\").str.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_dataframe['institution'] = emails_dataframe['address'].str.split(\"@\").str.get(1)\n",
    "emails_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_dataframe['institution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails_dataframe.to_csv(\"email-data-with-institution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Playing with time on real data\n",
    "\n",
    "* Let's look at the [311 data for the city of Pittsburgh](https://data.wprdc.org/dataset/311-data) from the WPRDC\n",
    "* You can  download the CSV file [here](https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571) or run the cell below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download 311 data\n",
    "!wget -nv https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571 -O 311.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 311 data directly from the WPRDC and parse dates directly\n",
    "pgh_311_data = pd.read_csv(\"311.csv\",\n",
    "                           index_col=\"CREATED_ON\", \n",
    "                           parse_dates=True)\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgh_311_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that the dataframe has been indexed by time we can select 311 complains by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 311 complaints on November 13th 2017\n",
    "pgh_311_data['2017-11-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the times just around new years celebration of 2016\n",
    "pgh_311_data[\"2015-12-31 20:00:00\":\"2016-01-01 02:00:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Someone clearly had a very roudy new years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping time with the `resample` method\n",
    "\n",
    "* You use the `resample()` method to *split* time into groups\n",
    "* Then you can *apply* the regular aggregation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of complaints per quarter...\n",
    "# note this doesn't make sense, but works anyway\n",
    "pgh_311_data.resample(\"Q\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of complaints per month\n",
    "pgh_311_data.resample(\"M\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok, these data are *begging* to be visualized, so let me show you one last feature of Pandas...Visalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load up the data visualization libraries\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph of the monthly complaint counts\n",
    "pgh_311_data['REQUEST_ID'].resample(\"M\").count().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "* I highly recommend this book, it covers NumPy, pandas, matplotlib, and scikit-learn. \n",
    "* It is well written and up to date!\n",
    "\n",
    "![Python Data Science Handbook](https://covers.oreillystatic.com/images/0636920034919/lrg.jpg)\n",
    "\n",
    "* If you want to go deeper into pandas, you can't do better than this book. \n",
    "* The 2nd edition just came out and it is written by Wes McKinney, the creator of pandas!\n",
    "\n",
    "![Python Data Analysis: *2nd Edition*](https://covers.oreillystatic.com/images/0636920050896/lrg.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
